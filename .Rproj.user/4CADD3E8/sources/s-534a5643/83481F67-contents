---
title: "Making Magic with Keras and Shiny"
subtitle: "An exploration of Shiny's position in the data science pipeline"
author: "Nick Strayer"
date: "2018/01/22"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

class: middle

# Outline

Three sections

1. What...
2. Why...
3. How...

... I did

---
class: center, middle

# What?

---

# Made a shiny app for casting spells

---

# Three main steps:

1. Shiny app for gathering data
2. Modeling data with Keras
3. Shiny app for presenting data

---

# Demo

---
class: center, middle

# Why?

---

I want to show Shiny can fit into the datascience workflow from the begining to the end. 

Traditionally shiny has been used to show off results afterwards, but that's missing a huge portion of its potential.

---

# Shiny for gathering data

Who is better at knowing how they need data than the ones who are going to be using it?

Many people like statisticians are given entire courses on how to set up things like clinical trials, but datascience doesn't always work like that.

---

# Shiny for presenting models

This is the traditional way shiny is used, but there's a reason for that. It's great. 

Instead of having to get fancy with learning how to port tensorflow models to Swift or Java you can just throw a predict function inside of a simple app. 

How well can this scale?

---
class: center, middle

# How?

---
class: center, middle

# Data gathering app


---

# Shiny

> Shiny is an R package that makes it easy to build interactive web apps straight from R.

[RStudio](https://shiny.rstudio.com/)
---

# App needs

- Gather data from accelerometer
- Save data somewhere I could use it
- Visualize results so that I could make sure I wasn't recording bad data.

---

#Gathering Data from accelerometer

Used my package `shinysense`. 

Binds to the `devicemotion` api in javascript to pull in motion data.

Needs to be noramlized for different devices.

---
# Saving the data

Two options: 
1. Use shinyapps.io and do a dropbox or similar upload
2. Just host the app from my own server
    - This was easier.
    - Better for other reasons I will get to later as well.
    
Luckily the RStudio Server makes this crazy easy.

---
# Visualizing data 

Not that hard as `shinysense::movr` returns an observable that can be used like any other shiny object.

Just used ggplot2 to visualize raw data faceting by recording.

Added a delete button if bad data was detected.

---
class: center, middle

# Data exploration/ Visualization

---
# Getting data all in

By saving all of the data to the hope repo as unique id `.csv`s all I have to do is read them in with a single `purrr` line. 

```{r, eval = FALSE}
all_files <- list.files('gather_data')
csv_names <- all_files[grepl('.csv', all_files)]

# load all data into a dataframe
data <- csv_names %>% 
  map_df(read_csv)
```


---
# Visualizing all data for trends

```{r, include = FALSE}
library(tidyverse)
data <- read_csv('spell_data.csv')
```

```{r, fig.width = 10, fig.height = 5}
ggplot(data, aes(x = time, y = accel, color = direction, group = recording_id)) +
  geom_line(alpha = 0.15) +
  facet_wrap(~label) +
  theme_void() +
  guides(color = FALSE) +
  theme(strip.text = element_text(family = "Amatic SC", size = 12))
```


---
# Converting data to tensors

Currently the largest sticking point in the R world for deep learning

We like to think flat and tidy, tensors do not.

```{r, eval = FALSE}
directions <- c('m_x','m_y','m_z')
wide_data <- data %>% 
  spread(direction, accel)
  group_by(label, recording_id) %>% 
  do( data = as.matrix(.[directions]) )
```

One-hot encoding is even more messy at this point. 

---
# Building Keras model

When building a deep learning model you need to ask yourself three things

1. What architecture fits my problem/limitations best?
2. How do I stack my layers?
3. Couldn't KNN do this better?


---

# Architecture

For sequential data there are two options, recurrent and convolutional. 

Reccurent are fantastic for learning time dependent patterns but are slow

Convolutional are great at recognizing patterns and are fast

Since I don't have a ton of compute and accelerometer data is often pattern recognized I chose a CNN

---
# Model Structure

A stack of a single convolutional layer with batch norm, max-pooling then lots of dropout.

Started simple and... didn't need to scale up.
---

# R's Keras vs Python's

This is where R shines in deep learning. 

The pipe operator turns the sequential API into a much more readable format than in python.

.pull-left[
__R__
```{r, eval = FALSE}
model <- keras_model_sequential()
model %>% 
  layer_conv_1d(...) %>%
  layer_batch_normalization() %>% 
  layer_global_max_pooling_1d() %>%
  layer_dropout(0.4) %>% 
  layer_dense(
    num_classes, 
    activation = 'softmax'
  ) 
```
]
.pull-right[
__Python__
```{python, eval = FALSE}
model = Sequential()
model.add(conv_1d(...))
model.add(batch_normalization)
model.add(global_max_pooling_1d)
model.add(dropout(0.4))
model.add(dense(
  num_classes,
  activation = 'softmax'
))
```
]



---
# Saving model for future use

You can dump the keras model as its own file to be loaded up and instantly used later. 

While you technically could do this with any model using `saveRds` it's nice to have it be a first-class citizen.

---
class: center, middle

# Modeling

---
class: center, middle

# Final Shiny app

---
# Converted original app

---
# Wire in Keras

Make sure to turn of GPU mode

---
# Testing it

---
# Making it look pretty

---
# Deploying it

